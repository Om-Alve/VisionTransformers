{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "157e96f6-5b18-4a29-8a24-bcbdf5a2235b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Patching(nn.Module):\n",
    "    \"\"\"\n",
    "    Patching module for extracting patches from input images.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, patch_size=4, embedding_dim=48):\n",
    "        super().__init__()\n",
    "        self.patch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embedding_dim,\n",
    "                      kernel_size=(patch_size, patch_size),\n",
    "                      stride=(patch_size, patch_size)),\n",
    "            nn.Flatten(2, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Patching module.\n",
    "        \"\"\"\n",
    "        return self.patch(x).transpose(-2, -1)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Head module for performing self-attention on input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.qkv = nn.Linear(n_embed, head_size * 3, bias=False)\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Head module.\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=2)\n",
    "        w = torch.bmm(k, q.transpose(-2, -1)) * (self.n_embed ** -0.5)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(-1).float()\n",
    "            w = w * attention_mask\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.attention_dropout(w)\n",
    "        out = torch.bmm(w, v)\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiHead module for combining multiple attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size, n_heads, n_embed,dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, dropout) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the MultiHead module.\n",
    "        \"\"\"\n",
    "        out = torch.cat([head(x, attention_mask) for head in self.heads], -1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    FeedForward module for applying a feed-forward neural network to input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embed * mlp_ratio, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the FeedForward module.\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Block module for a single transformer block.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, head_size, n_heads, dropout, mlp_ratio):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHead(head_size, n_heads, n_embed,dropout)\n",
    "        self.ffwd = FeedForward(n_embed, mlp_ratio, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Block module.\n",
    "        \"\"\"\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.multihead(x, attention_mask)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, patch_size=4, embedding_dim=48, head_size=12,\n",
    "                 n_heads=4, n_layers=2, dropout=0.4, mlp_ratio=2, block_size=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = Patching(in_channels, patch_size, embedding_dim)\n",
    "        self.positional_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        self.blocks = nn.ModuleList([Block(embedding_dim, head_size, n_heads, dropout, mlp_ratio) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.sequence_pooling = nn.Linear(embedding_dim,1)\n",
    "        self.cl_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * mlp_ratio, num_classes)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the ViT model.\n",
    "        \"\"\"\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x + self.positional_embedding(torch.arange(x.shape[1], device=x.device))\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask)\n",
    "        x = self.ln(x)\n",
    "        seq_pool = self.sequence_pooling(x).transpose(-2,-1) # B,1,N\n",
    "        seq_pool = torch.nn.functional.softmax(seq_pool,dim=2) # B,1,N\n",
    "        x = torch.bmm(seq_pool,x).squeeze(1) # B,D\n",
    "        x = self.cl_head(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf2006f8-6da9-4d25-9a1a-b805e78cf673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = ViT()\n",
    "out = model(torch.randn(32,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bdff576-0e3f-41ae-a439-0bc832e258a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a36ec-961a-47b4-9629-11f7260d4cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
