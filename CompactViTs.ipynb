{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "157e96f6-5b18-4a29-8a24-bcbdf5a2235b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "n_embed = 48\n",
    "head_size = 12\n",
    "n_heads = 4\n",
    "n_layers = 7\n",
    "dropout = 0.4\n",
    "mlp_ratio = 2\n",
    "device = 'cuda'\n",
    "block_size = 64\n",
    "\n",
    "class Patching(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 patch_size=4,\n",
    "                 embedding_dim=n_embed,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.patch = nn.Sequential(nn.Conv2d(in_channels, embedding_dim,\n",
    "                                             kernel_size=(patch_size, patch_size),\n",
    "                                             stride=(patch_size, patch_size),\n",
    "                                            ),\n",
    "                                   nn.Flatten(2, 3),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.patch(x).transpose(-2, -1)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # 32,87,40\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        # k -> 32,87,10\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        w = torch.bmm(k,q.transpose(-2, -1)) * (n_embed ** -0.5)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(-1).float()\n",
    "            w = w * attention_mask\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        out = torch.bmm(w,v)\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,head_size,n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "    def forward(self,x,attention_mask):\n",
    "        out = torch.cat([head(x,attention_mask) for head in self.heads],-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,n_embed * mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * mlp_ratio,n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHead(head_size,n_heads)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self,x,attention_mask):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.multihead(x,attention_mask)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = Patching()\n",
    "        self.positional_embedding = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.ModuleList([Block() for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(n_embed)\n",
    "        self.cl_head = nn.Sequential(\n",
    "            nn.Linear(n_embed,n_embed * mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * mlp_ratio,10)\n",
    "        )\n",
    "        self.sequence_pooling = nn.Linear(n_embed,1)\n",
    "    def forward(self,x,attention_mask=None,targets=None):\n",
    "        ini_emb = self.patch_embedding(x)\n",
    "        # ini_emb = torch.cat([ini_emb,self.class_embedding.expand(x.shape[0],-1,-1)],dim=1)\n",
    "        # B,1025,768\n",
    "        # b,t b=batch, t = tokens\n",
    "        B,N,S =  ini_emb.shape\n",
    "        pos_emb = self.positional_embedding(torch.arange(N,device=device))\n",
    "        x = ini_emb + pos_emb\n",
    "        # b,t,c = 1,1024,768\n",
    "        for block in self.blocks:\n",
    "            x = block(x,attention_mask)\n",
    "        x = self.ln(x) # B,N,D\n",
    "        seq_pool = self.sequence_pooling(x).transpose(-2,-1) # B,1,N\n",
    "        seq_pool = torch.nn.functional.softmax(out[1],dim=2) # B,1,N\n",
    "        x = torch.bmm(seq_pool,x).squeeze(1) # B,D\n",
    "        x = self.cl_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf2006f8-6da9-4d25-9a1a-b805e78cf673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = ViT()\n",
    "out = model(torch.randn(32,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bdff576-0e3f-41ae-a439-0bc832e258a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
