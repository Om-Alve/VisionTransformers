{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkgjbSytOZ5V",
        "outputId": "2c172f17-8de2-4f23-a553-cf946c61ca31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "cifar10_mean, cifar10_std = [0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(cifar10_mean, cifar10_std)])\n",
        "\n",
        "# Download and prepare the CIFAR-10 dataset\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders to efficiently load and iterate through the dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "n_embed = 128\n",
        "head_size = 32\n",
        "n_heads = 4\n",
        "n_layers = 4\n",
        "dropout = 0.4\n",
        "mlp_ratio = 2\n",
        "device = 'cuda'\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self,in_channels=3):\n",
        "        super().__init__()\n",
        "        self.patch = nn.Sequential(nn.Conv2d(in_channels, n_embed,\n",
        "                                             kernel_size=(3, 3),\n",
        "                                             stride=1,\n",
        "                                             padding=1,\n",
        "                                             bias=False,\n",
        "                                            ),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(kernel_size=(3,3),stride=2,padding=1),\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.patch(x).flatten(2,3).transpose(-2,-1)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.qkv = nn.Linear(n_embed, head_size * 3, bias=False)\n",
        "        self.attention_dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # 32,87,40\n",
        "        B,T,C = x.shape\n",
        "        q,k,v = self.qkv(x).chunk(3,dim=2)\n",
        "        # k -> 32,87,10\n",
        "        w = torch.bmm(k,q.transpose(-2, -1)) * (n_embed ** -0.5)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.unsqueeze(-1).float()\n",
        "            w = w * attention_mask\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = self.attention_dropout(w)\n",
        "        out = torch.bmm(w,v)\n",
        "        return out\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    def __init__(self,head_size,n_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_embed,n_embed)\n",
        "    def forward(self,x,attention_mask):\n",
        "        out = torch.cat([head(x,attention_mask) for head in self.heads],-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed,n_embed * mlp_ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(n_embed * mlp_ratio,n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.multihead = MultiHead(head_size,n_heads)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self,x,attention_mask):\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.multihead(x,attention_mask)\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.sequence_length = self.tokenizer(torch.randn(1,3,32,32)).shape[1]\n",
        "        self.blocks = nn.ModuleList([Block() for _ in range(n_layers)])\n",
        "        self.ln = nn.LayerNorm(n_embed)\n",
        "        self.cl_head = nn.Sequential(\n",
        "            nn.Linear(n_embed,10)\n",
        "        )\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1,self.sequence_length,n_embed,device=device),requires_grad=True)\n",
        "        self.sequence_pooling = nn.Linear(n_embed,1)\n",
        "    def forward(self,x,attention_mask=None,targets=None):\n",
        "        ini_emb = self.tokenizer(x)\n",
        "        B,N,S =  ini_emb.shape\n",
        "        pos_emb = self.positional_embedding\n",
        "        x = ini_emb + pos_emb\n",
        "        # b,t,c = 1,1024,768\n",
        "        for block in self.blocks:\n",
        "            x = block(x,attention_mask)\n",
        "        x = self.ln(x) # B,N,D\n",
        "        seq_pool = self.sequence_pooling(x).transpose(-2,-1) # B,1,N\n",
        "        seq_pool = torch.nn.functional.softmax(seq_pool,dim=2) # B,1,N\n",
        "        x = torch.bmm(seq_pool,x).squeeze(1) # B,D\n",
        "        x = self.cl_head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "q-FtmQjEOb1v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "model = ViT().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "rwsHeSt8OgpC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "from tqdm import tqdm\n",
        "accuracy = torch.tensor(0.0)\n",
        "num_epochs = 100\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=512, shuffle=True)\n",
        "for epoch in range(num_epochs+1):\n",
        "  loop = tqdm(train_loader,leave=False)\n",
        "  for x,y in loop:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    pred = model(x)\n",
        "    loss = criterion(pred,y)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_description(f\"Epoch : [{epoch}/{num_epochs}]\")\n",
        "    loop.set_postfix(loss=loss.item(),accuracy = accuracy.item())\n",
        "  if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        for x,y in test_loader:\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          pred = model(x)\n",
        "          pred = torch.argmax(pred,dim=1)\n",
        "          break\n",
        "        model.train();\n",
        "        accuracy = (pred == y).type(torch.int32).sum() / len(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "wmEo9fmbOilc",
        "outputId": "14f7e81c-6ded-4030-fb87-7601344e92b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-057f597f1da5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum([p.numel() for p in model.parameters()]) / 1e6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOFRw6QjOoAx",
        "outputId": "341f0b31-9854-4c5c-d9ae-f6b72ae4c9f6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.566283"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "xDhp4HsIbB9q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsBwe1bFcfzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}