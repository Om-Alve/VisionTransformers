{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2288da-bac5-4dee-a985-6f7aa64a9531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a5cfa-b6a9-4d9f-853a-f097e58722f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to preprocess the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),   # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "# Download and prepare the CIFAR-10 dataset\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders to efficiently load and iterate through the dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee23c3c-eb27-4f7f-bd11-e5777f3a7320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patching(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 patch_size=16,\n",
    "                 embedding_dim=768,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.patch = nn.Sequential(nn.Conv2d(in_channels, embedding_dim,\n",
    "                                             kernel_size=(patch_size, patch_size),\n",
    "                                             stride=(patch_size, patch_size),\n",
    "                                            ),\n",
    "                                   nn.Flatten(2, 3),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.patch(x).transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d09f3-7996-47df-b855-270722135b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Patching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a5c9b-9e04-44b3-943f-38ccfbe0957b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = np.array(Image.open('art.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e7062-05f9-46bc-b0f1-492ee1f9e887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_ = torch.tensor(img).permute(2,0,1).unsqueeze(0).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b498c1-2ad4-461f-b005-dc43b794aaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p(img_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02494e77-2ad4-4f45-880a-10bea64796bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1,512,512,3\n",
    "# 1,32,32,16,16\n",
    "# 1,1024,256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5f758-d39d-4fc2-85a0-bccbdaef8db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class Patching(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 patch_size=4,\n",
    "                 embedding_dim=48,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.patch = nn.Sequential(nn.Conv2d(in_channels, embedding_dim,\n",
    "                                             kernel_size=(patch_size, patch_size),\n",
    "                                             stride=(patch_size, patch_size),\n",
    "                                            ),\n",
    "                                   nn.Flatten(2, 3),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.patch(x).transpose(-2, -1)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        w = torch.bmm(k,q.transpose(-2, -1)) * (self.n_embed ** -0.5)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(-1).float()\n",
    "            w = w * attention_mask\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        out = torch.bmm(w,v)\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "    def forward(self,x,attention_mask):\n",
    "        out = torch.cat([head(x,attention_mask) for head in self.heads],-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,n_embed * mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * mlp_ratio,n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, n_heads, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHead(n_embed, head_size, n_heads)\n",
    "        self.ffwd = FeedForward(n_embed, mlp_ratio, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self,x,attention_mask):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.multihead(x,attention_mask)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=4, embedding_dim=48, head_size=12, n_heads=4, n_layers=7, dropout=0.4, mlp_ratio=2, device='cuda', block_size=64):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = Patching(in_channels, patch_size, embedding_dim)\n",
    "        self.positional_embedding = nn.Embedding(block_size+1, embedding_dim)\n",
    "        self.blocks = nn.ModuleList([Block(embedding_dim, head_size, n_heads, mlp_ratio, dropout) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "        self.class_embedding = nn.Parameter(torch.zeros(1, 1, embedding_dim),\n",
    "                                        requires_grad=True)\n",
    "        self.cl_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * mlp_ratio, 10)\n",
    "        )\n",
    "        self.sequence_pooling = nn.Linear(embedding_dim, 1)\n",
    "    def forward(self,x,attention_mask=None,targets=None):\n",
    "        ini_emb = self.patch_embedding(x)\n",
    "        ini_emb = torch.cat([ini_emb,self.class_embedding.expand(x.shape[0],-1,-1)],dim=1)\n",
    "        B,N,S =  ini_emb.shape\n",
    "        pos_emb = self.positional_embedding(torch.arange(N,device=device))\n",
    "        x = ini_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x,attention_mask)\n",
    "        x = self.ln(x)\n",
    "        x = x[:,0,:]\n",
    "        x = self.cl_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cd72a-ce73-44f8-b600-9c947593ba39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ViT(device='cpu')\n",
    "model(torch.randn(32,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23f597-71c4-4735-be9b-db24779a844d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = ViT().to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183e153-784b-46d9-8e43-408720038610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "from tqdm import tqdm\n",
    "accuracy = torch.tensor(0.0)\n",
    "num_epochs = 50\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(train_loader,leave=False)\n",
    "    for x,y in loop:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred,y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loop.set_description(f\"Epoch : [{epoch}/{num_epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item(),accuracy = accuracy.item())\n",
    "    if epoch % 2 == 0:\n",
    "        model.eval()\n",
    "        for x,y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(x)\n",
    "            pred = torch.argmax(pred,dim=1)\n",
    "            break\n",
    "        model.train();\n",
    "        accuracy = (pred == y).type(torch.int32).sum() / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61911ab-c7be-4108-ba17-8402ce68f91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
